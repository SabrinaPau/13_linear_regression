{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "\n",
    "This notebook is all about simple linear regression.  \n",
    "It includes a more in-depth explanation on linear regression, its equation and its application in Python.\n",
    "\n",
    "**Objectives**  \n",
    "At the end of the notebook you should be able to:\n",
    "- define the relationship between two variables\n",
    "- define the concepts of intercept and slope\n",
    "- reproduce the equation of Linear Regression\n",
    "- explain the concept behind OLS\n",
    "- build a linear regression model with the statsmodels module in Python\n",
    "- interpret the output of a linear regression model built with the statsmodels module\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/400px-Linear_regression.svg.png\" height=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In statistics, **linear regression** is an approach for modeling the relationship between  \n",
    "- a response/dependent variable $y$  \n",
    "- and one or more explanatory/independent variables $x_1, x_2, ..., x_n $.  \n",
    "\n",
    "Important: Linear regression can prove a relationship, but it **cannot prove causality**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Simple linear regression**: only one explanatory variable\n",
    "* **Multiple linear regression**: more than one explanatory variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's start with an example\n",
    "\n",
    "Suppose we want to explore the relationship between the fuel efficiency of a car (mpg, miles-per-gallon) and its weight in kilograms.\n",
    "\n",
    "We will use the data set `cars` which contains variables `mpg` and `weight`.  \n",
    "This dataset is downloaded from [vincentarelbundock's github](https://vincentarelbundock.github.io/Rdatasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:25.659140Z",
     "start_time": "2020-02-05T09:49:24.271595Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# this so called \"line magic\" command, amongst other things, stores the plots in the notebook document.\n",
    "%matplotlib inline\n",
    "\n",
    "# warnings supression\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip data (the trailing exclamation mark passes a command directly to the shell, not to the python interpreter,\n",
    "# so this works as if you'd execute the command to unzip the file in your terminal window)\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:26.361132Z",
     "start_time": "2020-02-05T09:49:26.084862Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# read in cars dataset and mark all questionmarks as NaNs.\n",
    "cars = pd.read_csv(\"data/cars_multivariate.csv\",\n",
    "                  na_values='?')\n",
    "cars.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As stated above, we want to model the relationship between `mpg` and `weight` with a straight line.  \n",
    "First, let's look at a scatterplot to get an idea about how our data is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:26.978524Z",
     "start_time": "2020-02-05T09:49:26.812838Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the variables of interest first\n",
    "cars.plot(x='weight', \n",
    "          y='mpg', \n",
    "          kind='scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there seems to be a negative correlation between a cars fuel efficiency and its weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When talking about modeling our data with a straight line, linear regression comes into play!  \n",
    "**Linear regression** is just the fancy term for **finding the line of best fit**.\n",
    "\n",
    "In other words, we are looking for the **slope** and **intercept** that defines a line that fits the data as well as possible, which mainly means that we are trying to minimize the sum of squared residuals (deviations of the estimated y-values from the true y-values)\n",
    "\n",
    "* **Intercept** - The value for $y$ when $x=0$ \n",
    "\n",
    "* **slope** - For each unit increase in $x$, the expected increase/decrease in $y$  \n",
    "(in the case of __multiple__ linear regression, we need to add \"holding all other explanatory variables constant\", since then there are more than one explanatory variable in the model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better understand the line equation of linear regression, we will start with finding the line between two points. \n",
    "This from-scratch method should help you understand the concepts of intercept and slope better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a function which takes two arbitrary points p1, p2 each in the format [x, y] to calculate slope and intercept of the line between the two points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:27.892716Z",
     "start_time": "2020-02-05T09:49:27.883025Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Function that returns slope and intercept\n",
    "def get_line_equation(p1, p2):\n",
    "    \"\"\"\n",
    "    Solve the system of equations:\n",
    "    y1 = m*x1 + b\n",
    "    y2 = m*x2 + b\n",
    "    \n",
    "    This translates to:\n",
    "    m = (y2 - y1) / (x2 - x1)\n",
    "    b = y1 - m*x1\n",
    "    \n",
    "    Input:\n",
    "    p1: first point [x1, y1]\n",
    "    p2: second point [x2, y2]\n",
    "    \n",
    "    returns: slope, intercept\n",
    "    \"\"\"\n",
    "    m = (p2[1] - p1[1]) / (p2[0] - p1[0]) # Slope y1-y2/x1-x2\n",
    "    b = p1[1] - m * p1[0] # Intercept\n",
    "    return  m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate the intercept and slope of a line between two observations of our dataset.  \n",
    "Therefore, we choose two points (here we use the first two observations of our dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:27.892716Z",
     "start_time": "2020-02-05T09:49:27.883025Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "car1 = [cars.weight[0], cars.mpg[0]]\n",
    "car2 = [cars.weight[1], cars.mpg[1]]\n",
    "\n",
    "print(f'point 1:{car1}, point 2:{car2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate the slope and intercept of the line between the chosen points by passing the points into our just created function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept = get_line_equation(p1=car1, p2=car2)\n",
    "\n",
    "print(f'intercept: {intercept}, slope: {slope}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got our intercept and slope for a line between the first two points of our dataset. Time to draw it onto our scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:28.223540Z",
     "start_time": "2020-02-05T09:49:28.081039Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the scatterplot again \n",
    "fig = cars.plot(x='weight', \n",
    "                y='mpg', \n",
    "                kind='scatter')\n",
    "\n",
    "# plot the line between the two points\n",
    "plt.plot([car1[0], car2[0]], [car1[1], car2[1]], color=\"red\", linewidth=4)\n",
    "\n",
    "# plot a title\n",
    "plt.title(\"Line fitted on two points\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may see, the red line between the two points p1 and p2 is probably not the best choice when taking all of our data points into account, at least the slope seems too steep at first glance.  \n",
    "We need to find new coefficients for the intercept and slope which resemble our whole data the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Simple linear regression formally defined\n",
    "Model assumption: y and x are linearly related.  \n",
    "\n",
    "General formula for a linear regression (this is a formula you should keep in mind):  \n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\varepsilon$$  \n",
    "where $\\beta_0, \\beta_1$ are the regression coefficients, $x$ is the independent variable and $\\varepsilon$ is the random error.  \n",
    "\n",
    "In linear regression, we try to find the _line of best fit_ that describes our data best.  \n",
    "\n",
    "The equation that takes all data into account looks like this:  \n",
    "$$y_i = \\hat{\\beta_0} + \\hat{\\beta_1} x_i + e_i$$  \n",
    "where $_i$ is the number of observations, $\\hat{\\beta_0} + \\hat{\\beta_1} x_i$ is considered the \"fit\" and $e_i$ is the residual error of every observation.\n",
    "\n",
    "Our data is modeled as \"fit + residual\" and we want to reduce the residuals $e_i$,  \n",
    "where $e_i = y_i - \\hat{y}_i$ is the difference between an observed value and the estimated value provided by a model,  \n",
    "and $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$  \n",
    "where $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are the estimated regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<left><img src=\"http://68.media.tumblr.com/7efec0c9403dd3b9eeb181e1681cd3d2/tumblr_inline_nii9j9oqS61rs8rb9.gif\" height=\"250\"/></left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "## Ordinary Least Squares (OLS)\n",
    "\n",
    "OLS is the simplest and most common way to estimate the intercept and the slope.\n",
    "\n",
    "It estimates by minimizing the sum of squared residuals: $\\sum_{i=1}^n e_i^2$\n",
    "\n",
    "Have a look at this [visual explanation](http://setosa.io/ev/ordinary-least-squares-regression/) for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "## Simple linear regression in Python \n",
    "\n",
    "In the next section you will build a linear regression model using one dependent and one independent variable from the cars dataset.\n",
    "\n",
    "We will use the [statsmodel.api](https://www.statsmodels.org/stable/api.html) module, which is a Python module that provides classes and functions for the estimation of many different statistical models. If you like, follow the link for a reference of available models, statistics, and tools.  \n",
    "\n",
    "There of course are different modules in python which you can perform OLS with.  \n",
    "We chose for this module because it gives us the option of easily printing a summary that will give us lots of statistical insights in our model itself.\n",
    "\n",
    "So let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:36.328583Z",
     "start_time": "2020-02-05T09:49:35.909214Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import the statsmodels.api module\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll prepare our data in a typical manner for modeling by chosing the dependent variable (in lowercase letter) and the independent variable (in uppercase letter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:36.358578Z",
     "start_time": "2020-02-05T09:49:36.342791Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the independent variable, the predictor X\n",
    "X = cars[['weight']]\n",
    "\n",
    "# Define dependent variable (since it's always a single variable, we can use dot notation here)\n",
    "y = cars.mpg\n",
    "\n",
    "display(X.head())\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Dive: Why do we have to add a constant?**  \n",
    "Statsmodel does not fit the intercept by default! Thus, if we don't add a constant, we won't have an adjustable intercept and our model is forced to let the regression line go through the origin of the coordinate system.\n",
    "The “const” column provides a placeholder — a bunch of 1s as constants which can be multiplied by $\\beta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our statsmodel OLS model needs an adjustable intercept, we add a column of 1s to:\n",
    "X = sm.add_constant(X)\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our data is prepared for modeling!  \n",
    "\n",
    "The following steps are:\n",
    "- creating a model based on the modules notation,\n",
    "- fitting the model to our data (this is the part where the sum of squared residuals is reduced to min - and passing the model results in a result variable)\n",
    "- print a summary with the models statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:49.416173Z",
     "start_time": "2020-02-05T09:49:49.396289Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create an OLS model\n",
    "our_model = sm.OLS(y, X)\n",
    "\n",
    "# use the data to calculate the intercept and slope\n",
    "model_results = our_model.fit()\n",
    "\n",
    "# return the output of the model\n",
    "model_results.summary() # summary contains eg. 'const' (intercept) and 'slope' of the regression equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the intercept and slope of your model by adressing the parameters via \".params\"\n",
    "# (you can also find it in the table above by looking at const coef and weight coef)\n",
    "intercept, slope = model_results.params\n",
    "\n",
    "print(f'intercept: {intercept}, slope: {slope}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Interpretation\n",
    "\n",
    "In the following you find the metrics which are most important for us.  \n",
    "For a __complete explanation of all of the metrics within the model__ summary check the __end of the notebook__.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ols_results_1.png\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "R-squared, $\\textbf{R}^2$ - Proportion of the variation in $y$ that is explained by the model. Measured on a scale from 0 (bad) to 1 (good)  \n",
    "**const coef** - This is the intercept, in other words: value of y when x = 0  \n",
    "**weight coef** - This is the slope, in other words: Amount, y changes for each unit change of x  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finalize the modeling part, we will plot our data again and additionally plot our final regression line by using the intercept and the slope from our OLS model results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of our model\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "x = cars['weight']\n",
    "y = cars['mpg']\n",
    "\n",
    "# add data points\n",
    "ax.scatter(x, y, alpha=0.5, color='orchid')\n",
    "fig.suptitle('Relationship between weight and mpg')\n",
    "\n",
    "# plotting the regression line with the help of our calculated intercept and slope variables\n",
    "ax.plot(x, x*slope+intercept, '-', color='darkorchid', linewidth=2);\n",
    "ax.set_ylabel(\"mpg\");\n",
    "ax.set_xlabel(\"weight\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "## Try it out yourself\n",
    "\n",
    "We added a constant earlier to our linear regression model. \n",
    "What happens if we don't use one? Is our model describing the dependent variable better or worse?\n",
    "\n",
    "Try out at least one other independent variable to explain `mpg` with a simple linear regression.\n",
    "Also try to plot the points and the regression line using to code just above.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When doing repeatitive work, always think about parametrising your work i.e. building functions:\n",
    "\n",
    "def create_fit_model(X, y, add_intercept=None):\n",
    "    \"\"\" \n",
    "    Returns the model summary, the intercept and the slope of a fitted model \n",
    "    \n",
    "    If the argument `add_intercept` isn't passed in, no intercept is added to the model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.core.frame.DataFrame\n",
    "        The independant variable, also called feature\n",
    "    y:  pandas.core.series.Series\n",
    "        The independant or target variable\n",
    "    add_intercept: boolean, optional\n",
    "        If True, an intercept is added to the model\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    model summary, the intercept and the slope\n",
    "    \"\"\"\n",
    "    \n",
    "    # adding an intercept\n",
    "    if add_intercept:\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "    # create an OLS model\n",
    "    model = sm.OLS(y, X)\n",
    "    \n",
    "    # use the data to calculate the intercept and slope\n",
    "    model_results = model.fit()\n",
    "    \n",
    "    # Getting the intercept and slope of the model\n",
    "    if add_intercept:\n",
    "        intercept, slope = model_results.params\n",
    "    else:\n",
    "        intercept = 0\n",
    "        slope = model_results.params[0]\n",
    "\n",
    "    return model_results.summary(), intercept, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(x, y, intercept, slope, x_label=None):\n",
    "    \"\"\" \n",
    "    Plots the regression line along with the scatter plots of x and y\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : pandas.core.series.Series\n",
    "        The independant variable, also called feature\n",
    "    y:  pandas.core.series.Series\n",
    "        The independant or target variable\n",
    "    intercept: float\n",
    "        The intercept for the regression line\n",
    "    slope: float\n",
    "        The slope for the regression line\n",
    "    x_label: str, optional\n",
    "        The label for the x variable on the graph\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plotting the results of our model\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    # add data points\n",
    "    ax.scatter(x, y, alpha=0.5, color='orchid')\n",
    "    fig.suptitle(f'Relationship between {x_label} and mpg')\n",
    "\n",
    "    # plotting the regression line with the help of our calculated intercept and slope variables\n",
    "    ax.plot(x, x*slope+intercept, '-', color='darkorchid', linewidth=2);\n",
    "    ax.set_ylabel(\"mpg\");\n",
    "    ax.set_xlabel(x_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined these handy functions, let's use them:\n",
    "\n",
    "# First, let's try the regression model without the intercept:\n",
    "\n",
    "X = cars[['weight']]\n",
    "y = cars.mpg\n",
    "x = cars['weight']\n",
    "\n",
    "model_summary, intercept, slope = create_fit_model(X, y)\n",
    "plot_regression(x, y, intercept, slope, \"weight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, forcing the intercept to be 0 is not a good idea. Indeed, the mpg values are high for low values of weight. In addition, mpg decreases as weight increases. Therefore, forcing the intercept to be 0 leads the regression line to be completely wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have automated our work thanks to the functions, we can try another variable easily.\n",
    "# For example, let's try with displacement\n",
    "\n",
    "# First, let's plot the variables to have an idea of how the data look like, \n",
    "# if regression makes sense and if adding an intercept makes sense too!\n",
    "\n",
    "# Plot the variables of interest first\n",
    "cars.plot(x='displacement', \n",
    "          y='mpg', \n",
    "          kind='scatter');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As we can see, the relation seems to be negatively correlated just like with weight.\n",
    "# Since there are high values of mpg for low value of displacement, adding an intercept makes sense\n",
    "\n",
    "# So, let's fit this regression\n",
    "\n",
    "# Define the variables\n",
    "# I'm even too lazy to repeat the variable name... so I will automate that part too!\n",
    "# Next time, there will be only one line to change!\n",
    "x_name = 'displacement'\n",
    "X = cars[[x_name]]\n",
    "y = cars.mpg\n",
    "x = cars[x_name]\n",
    "\n",
    "# Fit the model (note that we add the intercept in the parameters of the function)\n",
    "model_summary, intercept, slope = create_fit_model(X, y, add_intercept=True)\n",
    "\n",
    "# Plot the graph\n",
    "plot_regression(x, y, intercept, slope, x_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Linear regression is finding the best fit line to data\n",
    "- Simple linear regression is 1 outcome/dependent and 1 explanatory/independent variable\n",
    "- Linear regression can prove a relationship, but it **cannot prove causality**\n",
    "- Ordinary Least Squares (OLS) is the fitting algorithm, which mathematically finds the best fitting line to our data\n",
    "- $\\textbf{R}^2$ is the proportion of the variation explained by the model\n",
    "\n",
    "Good resource: [realpython.com: Linear Regression](https://realpython.com/linear-regression-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive: Model Summary\n",
    "\n",
    "In the following, the model output is explained in more detail. Remember the metrics which are most important for us in the first place though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can find the R-Squared, which is 0.693 i.e. very highly related\n",
    "* You can also look at the coefficients of the model for intercept and slope (next to \"weight\")\n",
    "* Kurtosis and Skew values are shown here\n",
    "* A lot of significance testing is being done here\n",
    "\n",
    "\n",
    "**Here is a brief description of these measures:**\n",
    "\n",
    "The left part of the first table gives some specifics on the data and the model:\n",
    "\n",
    "* **Dep. Variable**: Singular. Which variable is the point of interest of the model\n",
    "* **Model**: Technique used, an abbreviated version of Method (see methods for more).\n",
    "* **Method**: The loss function optimized in the parameter selection process. Least Squares since it picks the parameters that reduce the training error. This is also known as Mean Square Error [MSE].\n",
    "* **No. Observations**: The number of observations used by the model, or size of the training data.\n",
    "* **Degrees of Freedom Residuals**: Degrees of freedom of the residuals, which is the number of observations – number of parameters. Intercept is a parameter. The purpose of Degrees of Freedom is to reflect the impact of descriptive/summarizing statistics in the model, which in regression is the coefficient. Since the observations must \"live up\" to these parameters, they only have so many free observations, and the rest must be reserved to \"live up\" to the parameters' prophecy. This internal mechanism ensures that there are enough observations to match the parameters.\n",
    "* **Degrees of Freedom Model**: The number of parameters in the model (not including the constant/intercept term if present)\n",
    "* **Covariance Type**: Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process. Since this model is Ordinary Least Squares, it is non-robust and therefore highly sensitive to outliers.\n",
    "\n",
    "The right part of the first table shows the goodness of fit \n",
    "\n",
    "* **R-squared**: The coefficient of determination, the Sum Squares of Regression divided by Total Sum Squares. This translates to the percent of variance explained by the model. The remaining percentage represents the variance explained by error, the E term, the part that model and predictors fail to grasp.\n",
    "* **Adj. R-squared**: Version of the R-Squared that penalizes additional independent variables. \n",
    "* **F-statistic**: A measure of how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals. Feeds into the calculation of the P-Value.\n",
    "* **Prob (F-statistic) or P-Value**: The probability that a sample like this would yield the above statistic, and whether the model's verdict on the null hypothesis will consistently represent the population. Does not measure effect magnitude, instead measures the integrity and consistency of this test on this group of data.\n",
    "* **Log-likelihood**: The log of the likelihood function.\n",
    "* **AIC**: The Akaike Information Criterion. Adjusts the log-likelihood based on the number of observations and the complexity of the model. Penalizes the model selection metrics when more independent variables are added.\n",
    "* **BIC**: The Bayesian Information Criterion. Similar to the AIC, but has a higher penalty for models with more parameters. Penalizes the model selection metrics when more independent variables are added.\n",
    "\n",
    "Second Table: Coefficient Reports \n",
    "\n",
    "* **coef**: The estimated value of the coefficient. By how much the model multiplies the independent value by.\n",
    "* **std err**: The basic standard error of the estimate of the coefficient. Average distance deviation of the points from the model, which offers a unit relevant way to gauge model accuracy.\n",
    "* **t**: The t-statistic value. This is a measure of how statistically significant the coefficient is.\n",
    "* **P > |t|**: P-value that the null-hypothesis that the coefficient = 0 is true. If it is less than the confidence level, often 0.05, it indicates that there is a statistically significant relationship between the term and the response.\n",
    "* **[95.0% Conf. Interval]**: The lower and upper values of the 95% confidence interval. Specific range of the possible coefficient values.\n",
    "\n",
    "Third Table: Residuals, Autocorrelation, and Multicollinearity \n",
    "\n",
    "* **Skewness**: A measure of the symmetry of the data about the mean. Normally-distributed errors should be symmetrically distributed about the mean (equal amounts above and below the line). The normal distribution has 0 skew.\n",
    "* **Kurtosis**: A measure of the shape of the distribution. Compares the amount of data close to the mean with those far away from the mean (in the tails), so model \"peakiness\". The normal distribution has a Kurtosis of 3, and the greater the number, the more the curve peaks.\n",
    "* **Omnibus D’Angostino’s test**: It provides a combined statistical test for the presence of skewness and kurtosis.\n",
    "* **Prob(Omnibus)**: The above statistic turned into a probability\n",
    "* **Jarque-Bera**: A different test of the skewness and kurtosis\n",
    "* **Prob (JB)**: The above statistic turned into a probability\n",
    "* **Durbin-Watson**: A test for the presence of autocorrelation (that the errors are not independent), which is often important in time-series analysis\n",
    "* **Cond. No**: A test for multicollinearity (if in a fit with multiple parameters, the parameters are related to each other).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep-dive into $\\textbf{R}^2$ \n",
    "\n",
    "How to calculate $\\textbf{R}^2$:\n",
    "\n",
    "$R^2 = \\frac{SSR}{SST}$\n",
    "\n",
    "SST = SSR + SSE\n",
    "\n",
    "* Total sum of squares (**SST**): $\\sum_{i=1}^n (y_i - \\bar{y})^2$\n",
    "\n",
    "\n",
    "* Regression sum of squares (**SSR**): $\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2$\n",
    "\n",
    "\n",
    "* Residual sum of squares (**SSE**): $\\sum_{i=1}^n e_i^2$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "interpreter": {
   "hash": "52746facf0ff27bba74d90023bea822bf28860e85219d052de28537e22041b59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
