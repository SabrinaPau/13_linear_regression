{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Categorical Variables\n",
    "\n",
    "## Learning Objectives\n",
    "___\n",
    "At the end of this notebook you should be able to \n",
    "\n",
    "- define categorical variables\n",
    "\n",
    "- transform a categorical variable with label encoder and dummy variables\n",
    "\n",
    "- explain multicollinearity and avoid it when using dummy variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our cars dataset. As always, we will import the necessary libraries and load the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:32.709751Z",
     "start_time": "2020-02-05T14:30:32.102848Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/cars_multivariate.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the columns in our dataset\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:34.175679Z",
     "start_time": "2020-02-05T14:30:34.164575Z"
    }
   },
   "outputs": [],
   "source": [
    "# figure out which type the data is stored in\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for \"car name\", every other column seems to have numerical values. They could also be candidates to describe the dependent variable `mpg` (miles per gallon).\n",
    "Does this mean we donÂ´t have any categorical variables?\n",
    "\n",
    "**What are categorical variables again?**  \n",
    "Categorical variables are types of data which can be divided into groups.  \n",
    "We can figure out if variables might be categorical or numerical.\n",
    "As first example, let's take a closer look at the column \"origin\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:35.079201Z",
     "start_time": "2020-02-05T14:30:35.067808Z"
    }
   },
   "outputs": [],
   "source": [
    "# we want to explore the descriptives of that column\n",
    "print(data['origin'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:35.717083Z",
     "start_time": "2020-02-05T14:30:35.711726Z"
    }
   },
   "outputs": [],
   "source": [
    "# how many different values can be found in this column\n",
    "print(data['origin'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values range from 1 to 3, moreover, actually the only values that are in the dataset are 1, 2 and 3! it turns out that \"origin\" is a so-called **categorical** variable. It does not represent a continuous number but refers to a location - say 1 may stand for US, 2 for Europe, 3 for Asia (note: for this dataset the actual meaning is not disclosed).\n",
    "\n",
    "So, categorical variables are exactly what they sound like: they represent categories instead of numerical features. \n",
    "Note that even though that's not the case here, these features are often stored as text values which represent various levels of the observations.\n",
    "\n",
    "## Identifying categorical variables\n",
    "___\n",
    "As categorical variables need to be treated in a particular manner especially in ML algorithms, as you'll see later on, you need to make sure to identify which variables are categorical. \n",
    "\n",
    "\n",
    "\n",
    "In some cases, identifying will be easy (e.g. if they are stored as strings), then the datatype will return an object type (f.e. carname). In other cases they are numeric and the fact that they are categorical is not always immediately apparent (as origin, model and cylinders in our example).\n",
    "\n",
    "\n",
    "\n",
    "Note that this may not be trivial. A first thing you can do is use the `.describe()` and `.info()` methods. `.describe()` will give you info on the data types (like strings, integers, etc). But even then continuous variables might have been imported as strings, or categorical as numeric - so it's very important to really have a look at your data and plot them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:36.930224Z",
     "start_time": "2020-02-05T14:30:36.299225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot our variables in order to check their datatypes\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(16,3))\n",
    "\n",
    "for xcol, ax in zip(['acceleration', 'displacement', 'horsepower', 'weight'], axes):\n",
    "    data.plot(kind='scatter', x=xcol, y='mpg', ax=ax, alpha=0.4, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:37.267898Z",
     "start_time": "2020-02-05T14:30:36.950430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do the same for the variables cylinders, model and origin\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,3))\n",
    "\n",
    "for xcol, ax in zip([ 'cylinders', 'model', 'origin'], axes):\n",
    "    data.plot(kind='scatter', x=xcol, y='mpg', ax=ax, alpha=0.4, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the structural difference between the top and bottom set of graphs. You can tell the structure looks very different: instead of getting a pretty homogeneous \"cloud\", categorical variables generate vertical lines for discrete values. Another plot type that may be useful to look at is the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:38.124894Z",
     "start_time": "2020-02-05T14:30:37.351391Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.gca()\n",
    "data.hist(ax = ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in the histograms, the structural difference between categorical and continouus variables becomes clear: Since categorical variables take discrete values, the histograms look different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally to the plots, also look at the number of unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:38.861393Z",
     "start_time": "2020-02-05T14:30:38.847523Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[['cylinders', 'model', 'origin']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming categorical variables\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to use categorical variables in regression models, they need to be transformed. There are two approaches to this:\n",
    "1) Perform label encoding\n",
    "2) Create dummy variables / one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding is when you represent your labels as numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate label encoding and dummy creation with the following Pandas Series with 3 categories: \"USA\", \"EU\" and \"ASIA\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:40.311662Z",
     "start_time": "2020-02-05T14:30:40.306971Z"
    }
   },
   "outputs": [],
   "source": [
    "# create pandas series\n",
    "origin = ['USA', 'EU', 'EU', 'ASIA','USA', 'EU', 'EU', 'ASIA', 'ASIA', 'USA']\n",
    "origin_series = pd.Series(origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll want to make sure Python recognizes the strings as categories. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:41.325376Z",
     "start_time": "2020-02-05T14:30:41.314770Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform series as type category\n",
    "cat_origin = origin_series.astype('category')\n",
    "cat_origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the `dtype` (i.e., data type) here is `category` and the three categories are detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding with .cat.codes\n",
    "You'll perform label encoding in a way that numerical labels are always between 0 and (number_of_categories)-1. There are several ways to do this, one way is using `.cat.codes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:42.294098Z",
     "start_time": "2020-02-05T14:30:42.286981Z"
    }
   },
   "outputs": [],
   "source": [
    "# assign each category a numeric label with cat.codes\n",
    "cat_origin.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding with scikit-learn's LabelEncoder\n",
    "Another way is to use scikit-learn's `LabelEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:43.578525Z",
     "start_time": "2020-02-05T14:30:43.055178Z"
    }
   },
   "outputs": [],
   "source": [
    "# import module, det up LabelEncoder and fit_transform your pandas series\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "origin_encoded = lb_make.fit_transform(cat_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:43.602559Z",
     "start_time": "2020-02-05T14:30:43.599036Z"
    }
   },
   "outputs": [],
   "source": [
    "origin_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while `.cat.codes` can only be used on variables that are transformed using `.astype(category)`, this is not a requirement when you use `LabelEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Creating Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to transform categorical variables is through using one-hot encoding or \"dummy variables\". The idea is to convert each category into a new column, and assign a 1 or 0 to the column. There are several libraries that support one-hot encoding, let's take a look at two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:45.043885Z",
     "start_time": "2020-02-05T14:30:45.031386Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dummy variables with the help of pandas\n",
    "pd.get_dummies(cat_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the label name has become the column name! Another method is through using the `LabelBinarizer` in scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:45.889380Z",
     "start_time": "2020-02-05T14:30:45.875085Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dummy variables with the help of scikit-learn\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "origin_dummies = lb.fit_transform(cat_origin)\n",
    "# You need to convert this back to a dataframe\n",
    "origin_dum_df = pd.DataFrame(origin_dummies,columns=lb.classes_)\n",
    "origin_dum_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of using dummies is that, whatever algorithm you'll be using, your numerical values cannot be misinterpreted as being continuous. Going forward, it's important to know that for linear regression (and most other algorithms in scikit-learn), **one-hot encoding is required** when adding categorical variables in a regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dummy Variable Trap\n",
    "___\n",
    "\n",
    "Due to the nature of how dummy variables are created, one variable can be predicted from all of the others. This is known as perfect **multicollinearity** and it can be a problem for regression. If this isn't super clear, go back to the one-hot encoded origin data above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:46.775828Z",
     "start_time": "2020-02-05T14:30:46.765412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trap_df = pd.get_dummies(cat_origin)\n",
    "trap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence of creating dummy variables for every origin, you can now predict any single origin dummy variable using the information from all of the others. OK, that might sound more like a tongue twister than an explanation.  \n",
    "Let's look at an example: Focus on the ASIA column for now. You can perfectly predict this column by adding the values in the EU and USA columns then subtracting the sum from 1 as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:47.525576Z",
     "start_time": "2020-02-05T14:30:47.511423Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict ASIA column from EU and USA\n",
    "predicted_asia = 1 - (trap_df['EU'] + trap_df['USA'])\n",
    "predicted_asia.to_frame(name='Predicted_ASIA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EU and USA can be predicted in a similar manner which you can work out on your own. \n",
    "\n",
    "You are probably wondering why this is a problem for regression. Recall that the coefficients derived from a regression model are used to make predictions. In a multiple linear regression, the coefficients represent the average change in the dependent variable for each 1 unit change in a predictor variable, assuming that all the other predictor variables are kept constant. This is no longer the case when predictor variables are related which, as you've just seen, happens automatically when you create dummy variables. This is what is known as the **Dummy Variable Trap**.\n",
    "\n",
    "Fortunately, the dummy variable trap can be avoided by simply dropping one of the dummy variables. You can do this by subsetting the dataframe manually or, more conveniently, by passing ```drop_first=True``` to ```get_dummies()```: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:48.311279Z",
     "start_time": "2020-02-05T14:30:48.299428Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dummy variables and drop first column to avoid multicollinearity\n",
    "pd.get_dummies(cat_origin, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a close look at the DataFrame above, you'll see that there is no longer enough information to predict any of the columns so the multicollinearity has been eliminated. \n",
    "\n",
    "You'll soon see that dropping the first variable affects the interpretation of regression coefficients. The dropped category becomes what is known as the **reference category**. The regression coefficients that result from fitting the remaining variables represent the change *relative* to the reference.\n",
    "\n",
    "You'll also see that in certain contexts, multicollinearity and the dummy variable trap are less of an issue and can be ignored. It is therefore important to understand which models are sensitive to multicollinearity and which are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variables in practice: Back to our auto-mpg data\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and change our \"cylinders\", \"model\", and \"origin\" columns over to dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:49.790006Z",
     "start_time": "2020-02-05T14:30:49.780202Z"
    }
   },
   "outputs": [],
   "source": [
    "cyl_dummies = pd.get_dummies(data['cylinders'], prefix='cyl', drop_first=True)\n",
    "yr_dummies = pd.get_dummies(data['model'], prefix='yr', drop_first=True)\n",
    "orig_dummies = pd.get_dummies(data['origin'], prefix='orig', drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results - also have a look at your other dummy dfs\n",
    "cyl_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's remove the original columns from our data and add the dummy columns instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:50.565101Z",
     "start_time": "2020-02-05T14:30:50.560167Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(['cylinders','model','origin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T14:30:51.089689Z",
     "start_time": "2020-02-05T14:30:51.063606Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.concat([data, cyl_dummies, yr_dummies, orig_dummies], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build multiple regression model including categorical variables\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to use these variables in our regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use all of of variables in our regression model, except mpg (since it's our dependent variable) and car_name\n",
    "variables = data.columns.to_list()\n",
    "variables.remove('mpg')\n",
    "variables.remove('car_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[variables]\n",
    "y = data.mpg\n",
    "X = sm.add_constant(X)\n",
    "res = sm.OLS(y, X).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understanding of the interpretation of the dummy coefficients, have a look at [this website](https://dss.princeton.edu/online_help/analysis/dummy_variables.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do with the not-significant dummy variables? \n",
    "A good answer to this question was found [here](https://www.quora.com/Is-it-necessary-to-include-a-dummy-variable-in-regression-analysis-when-it-is-not-significant):  \n",
    "\"The estimated coefficients of the included variables represent differences between their effects and those of the one that was left out. If one of the dummy variables has an insignificant coefficient, this only means that its effect is nearly the same as that of the one that was left out. If you choose a different dummy variable to leave out, the coefficients and apparent significance of all the others could change.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "___\n",
    "You now know ...\n",
    "- ... that you need to look closely at your data to **find categorical variables**. You can do so by looking at the info and descriptive statistics as well as by figuring out how many unique values each variable has. Also, scatter plots and histograms might help to find categorical variables.  \n",
    "- ... how to transform a variable with **label encoder** and **dummy variables**. For both ways you can use for example the [scikit-learns preprocessing tools](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features).  \n",
    "- ... what **multicollinearity** is and how to **avoid it when using dummy variables**.  \n",
    "\"Multicollinearity refers to a situation in which more than two explanatory variables in a multiple regression model are highly linearly related. We have perfect multicollinearity if [...] the correlation between two independent variables is equal to 1 or â1.\" [Multicolinearity Wikipedia](https://en.wikipedia.org/wiki/Multicollinearity#:~:text=Multicollinearity%20refers%20to%20a%20situation,equal%20to%201%20or%20%E2%88%921.).  \n",
    "To avoid collinearity when using dummy variables, you need to drop one dummy-column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice time\n",
    "___\n",
    "Can you find a combination of variables (also dummy variables) that beat your last multiple regression model from Notebook 4?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52746facf0ff27bba74d90023bea822bf28860e85219d052de28537e22041b59"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nf_base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
